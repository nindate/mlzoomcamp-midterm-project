{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "## Table of Contents\n",
    "\n",
    "* [1. About the project](#about_project)\n",
    "* [2. Import libraries and load data](#import_libraries_load_data)\n",
    "* [3 Exploratory data analysis (EDA)](#eda)\n",
    "  * [3.1 EDA - basic](#eda-basic)\n",
    "  * [3.2 EDA - additional](#eda-additional)\n",
    "* [4 Baseline model](#baseline-model)\n",
    "* [5 Improvement over baseline model](#baseline-improvement)\n",
    "  * [5.1 Logistic Regression](#logistic-regression)\n",
    "  * [5.2 Decision Tree](#decision-tree)\n",
    "  * [5.3 Random Forest](#random-forest)\n",
    "  * [5.4 XGBoost](#xgb)\n",
    "* [6. Final model](#final-model)\n",
    "  * [6.1 Compare results from hyper-parameter tuning for the different models and choose final model](#choose-final-model)\n",
    "  * [6.2 Train final model](#train-final-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='about_project'></a>\n",
    "[back to TOC](#toc)\n",
    "### 1. About the project:\n",
    "\n",
    "Banks are important institutions that provide funds, in terms of loans, to businesses and individuals to function and prosper. However banks need money to provide as loan and to also make their own investments (e.g. in stocks). One such good source that banks have is in the from of Term Deposits that bank's customers make.\n",
    "\n",
    "Banks regularly make calls to their customers to secure such Term deposits. However, from a big list of all its customers, it would be wise to make calls to the customers who are more likely to invest. This way, banks can reduce the cost of acquisition of Term deposit (in the form of payment to staff making call, call charges and so on.).\n",
    "\n",
    "This project aims at building a machine learning model that can be trained from previous marketing campaigns and data collected, to predict customers that potentialy will subscribe to Term deposit with the bank. Further, the prediction model will be hosted as a web service, which can accept customer data (in JSON format) and return the prediction (whether customer is likely to subscribe to Term deposit).\n",
    "\n",
    "#### The dataset\n",
    "**Citation:** [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
    "\n",
    "*Data source:* https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "(Also available at https://www.openml.org/d/1461 but has some differences due to data being processed a bit)\n",
    "\n",
    "*Datafile to be used:* https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "\n",
    "* 1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n",
    "* 2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\n",
    "\n",
    "The bank-additional-full.csv (which has the complete dataset) is used for this project.\n",
    "\n",
    "#### Notes from data source\n",
    "\n",
    "##### Input variables:\n",
    "\n",
    "*bank client data:*\n",
    "* 1 - age (numeric)\n",
    "* 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "* 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "* 4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "* 5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "* 6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "* 7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "*related with the last contact of the current campaign:*\n",
    "* 8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "* 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "* 10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "* 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "*other attributes:*\n",
    "* 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "* 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "* 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "*social and economic context attributes*\n",
    "* 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "* 17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "* 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "* 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "* 20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "##### Output variable (desired target)\n",
    "* 21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import_libraries_load_data'></a>\n",
    "### 2. Import libraries and load data\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-24T07:47:34.785155Z",
     "iopub.status.busy": "2021-10-24T07:47:34.784846Z",
     "iopub.status.idle": "2021-10-24T07:47:34.796697Z",
     "shell.execute_reply": "2021-10-24T07:47:34.795783Z",
     "shell.execute_reply.started": "2021-10-24T07:47:34.785123Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:02.9273Z",
     "iopub.status.busy": "2021-10-24T07:35:02.927033Z",
     "iopub.status.idle": "2021-10-24T07:35:03.094808Z",
     "shell.execute_reply": "2021-10-24T07:35:03.093906Z",
     "shell.execute_reply.started": "2021-10-24T07:35:02.927269Z"
    }
   },
   "outputs": [],
   "source": [
    "datafile = 'bank-additional-full.csv'\n",
    "df = pd.read_csv(datafile,delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.097677Z",
     "iopub.status.busy": "2021-10-24T07:35:03.097236Z",
     "iopub.status.idle": "2021-10-24T07:35:03.152705Z",
     "shell.execute_reply": "2021-10-24T07:35:03.151676Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.097616Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "### 3. Exploratory Data Analysis\n",
    "\n",
    "[back to TOC](#toc)\n",
    "\n",
    "This section performs various analysis of the dataset, split it into training, validation, test.\n",
    "\n",
    "* [3.1 EDA - basic](#eda-basic)\n",
    "* [3.2 EDA - additional](#eda-additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda-basic'></a>\n",
    "#### 3.1  EDA - Basic\n",
    "\n",
    "* Check if columns are correctly classified as numerical and categorical *(sometimes numerical columns are marked categorical or vice versa)*\n",
    "* Check for missing data and impute if data is missing\n",
    "* Check if any numerical features have extremely high values *(sometimes NaNs are coded as high number like 99999999)*\n",
    "* Check cardinality of categorical features *(if very high cardinality then using one-hot encoding may create a lot of features)*\n",
    "* Target variable analysis \n",
    "  - Convert categorical to binary *(since this dataset has target values of yes and no)*\n",
    "  - Check whether there is class imbalance *(if class imbalance then accuracy should not be used as evaluation metric, rather roc_auc can be used)*\n",
    "* Several features have 'unknown' as values *(representative of NaNs)*. Check whether these samples can be deleted or it will reduce the dataset significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the columns have correct data type (sometimes numerical columns are marked categorical or vice versa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.154998Z",
     "iopub.status.busy": "2021-10-24T07:35:03.154371Z",
     "iopub.status.idle": "2021-10-24T07:35:03.17173Z",
     "shell.execute_reply": "2021-10-24T07:35:03.170968Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.154951Z"
    }
   },
   "outputs": [],
   "source": [
    "df_head = df.head(2).T\n",
    "dtypes = list(df.dtypes.values)\n",
    "df_head.insert(loc=0,column='dtype',value=dtypes)\n",
    "df_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** The columns seem to be correctly typed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.173148Z",
     "iopub.status.busy": "2021-10-24T07:35:03.172906Z",
     "iopub.status.idle": "2021-10-24T07:35:03.23868Z",
     "shell.execute_reply": "2021-10-24T07:35:03.237699Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.173121Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.259274Z",
     "iopub.status.busy": "2021-10-24T07:35:03.258696Z",
     "iopub.status.idle": "2021-10-24T07:35:03.268819Z",
     "shell.execute_reply": "2021-10-24T07:35:03.268219Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.25924Z"
    }
   },
   "outputs": [],
   "source": [
    "#Numerical and Categorical features\n",
    "t_num_cols = list(df.columns[df.dtypes != 'object'])\n",
    "t_cat_cols = list(df.columns[df.dtypes == 'object'])\n",
    "t_cat_cols.remove('y')\n",
    "display(t_num_cols)\n",
    "display(t_cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any of the numerical columns have significantly high values (sometimes NaNs are filled as something like 99999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.27173Z",
     "iopub.status.busy": "2021-10-24T07:35:03.271013Z",
     "iopub.status.idle": "2021-10-24T07:35:03.327821Z",
     "shell.execute_reply": "2021-10-24T07:35:03.327122Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.271693Z"
    }
   },
   "outputs": [],
   "source": [
    "df[t_num_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Looking at the max for all the numerical features, we can see that there are no significantly high values for any of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, check cardinality (distinct values and their count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.329293Z",
     "iopub.status.busy": "2021-10-24T07:35:03.328953Z",
     "iopub.status.idle": "2021-10-24T07:35:03.458544Z",
     "shell.execute_reply": "2021-10-24T07:35:03.457938Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.329264Z"
    }
   },
   "outputs": [],
   "source": [
    "df[t_cat_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.460082Z",
     "iopub.status.busy": "2021-10-24T07:35:03.459847Z",
     "iopub.status.idle": "2021-10-24T07:35:03.514278Z",
     "shell.execute_reply": "2021-10-24T07:35:03.513363Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.460051Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of distinct values for each categorical feature\n",
    "df[t_cat_cols].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features do not have high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.515995Z",
     "iopub.status.busy": "2021-10-24T07:35:03.515515Z",
     "iopub.status.idle": "2021-10-24T07:35:03.636953Z",
     "shell.execute_reply": "2021-10-24T07:35:03.636004Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.515959Z"
    }
   },
   "outputs": [],
   "source": [
    "#Distinct values and their count for each categorical feature\n",
    "for c in t_cat_cols:\n",
    "    print(c)\n",
    "    display(df[c].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cardinality for the categorical columns look to be ok, not too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting target variable from having 'yes'/'no' to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.638902Z",
     "iopub.status.busy": "2021-10-24T07:35:03.638493Z",
     "iopub.status.idle": "2021-10-24T07:35:03.658909Z",
     "shell.execute_reply": "2021-10-24T07:35:03.658004Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.638856Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y'] = (df['y'] == 'yes').astype(int)\n",
    "df['y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** There is a high class imbalance in the target variable - 89% (no) - 11% (yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the features have the value 'unknown' possibly since that piece of information was not available. Checking how many such records are there with unknowns and if removing these make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.661222Z",
     "iopub.status.busy": "2021-10-24T07:35:03.660469Z",
     "iopub.status.idle": "2021-10-24T07:35:03.796446Z",
     "shell.execute_reply": "2021-10-24T07:35:03.795265Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.661174Z"
    }
   },
   "outputs": [],
   "source": [
    "df_copy1 = df.copy()\n",
    "initial_records = df_copy1.shape[0]\n",
    "for c in t_cat_cols:\n",
    "    df_copy1.drop(df_copy1[df_copy1[c] == 'unknown'].index, inplace = True)\n",
    "final_records = df_copy1.shape[0]\n",
    "print(initial_records - final_records,(initial_records - final_records)/final_records)\n",
    "del df_copy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Thus totally there are 10700 records with atleast one feature having value as 'unknown'. This is 35% of the total data. So it does not make sense to remove this data. Will have to go ahead with the 'unknown' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "\n",
    "rows, columns = 2, 5\n",
    "n_row, n_col = 0, 0\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(20,10))\n",
    "for num_ft in t_num_cols:\n",
    "    if n_col < columns:\n",
    "        axes[n_row][n_col].hist(df[num_ft])\n",
    "        axes[n_row][n_col].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    else:\n",
    "        n_row = 1\n",
    "        axes[n_row][n_col-columns].hist(df[num_ft])\n",
    "        axes[n_row][n_col-columns].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see that the numerical features 'duration', 'campaign', 'pdays', 'previous' are not normally distributed. Will use this information for additional EDA and when performing experiments with models and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_transform = ['duration', 'campaign', 'pdays', 'previous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda-additional'></a>\n",
    "#### 3.2. EDA - additional\n",
    "[back to TOC](#toc)\n",
    "\n",
    "* Split the data into Train (70%), Validation (20%) and Test (10%)\n",
    "* For the features 'duration', 'campaign', 'pdays', 'previous' perform various transformations using transformers like Quantile, Power, Log transformation to see if the distribution can be brought close to normal\n",
    "* Feature importance - using mutual information score for categorical features and using correlation for numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data as Train (70%), Val (20%), Test (10%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.798327Z",
     "iopub.status.busy": "2021-10-24T07:35:03.797985Z",
     "iopub.status.idle": "2021-10-24T07:35:03.835577Z",
     "shell.execute_reply": "2021-10-24T07:35:03.834699Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.798274Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(df,test_size=0.1,shuffle=True,random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train,test_size=0.22,shuffle=True,random_state=1)\n",
    "print(f'train : {round(df_train.shape[0]/df.shape[0],2)}, val: {round(df_val.shape[0]/df.shape[0],2)}, test: {round(df_test.shape[0]/df.shape[0],2)}')\n",
    "df_test_project = df_test.copy()   #Dataframe with test features and label if required to use final model for preditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_full_train['y'].value_counts(normalize=True))\n",
    "display(df_train['y'].value_counts(normalize=True))\n",
    "display(df_val['y'].value_counts(normalize=True))\n",
    "display(df_test['y'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.837676Z",
     "iopub.status.busy": "2021-10-24T07:35:03.837166Z",
     "iopub.status.idle": "2021-10-24T07:35:03.861038Z",
     "shell.execute_reply": "2021-10-24T07:35:03.859911Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.837615Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = df_train['y'].values\n",
    "y_val = df_val['y'].values\n",
    "y_test = df_test['y'].values\n",
    "\n",
    "del df_train['y']\n",
    "del df_val['y']\n",
    "del df_test['y']\n",
    "\n",
    "df_full_train = df_full_train.reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "df_train_trans = pd.DataFrame(quantile_transformer.fit_transform(df_train[num_cols_transform]),columns=num_cols_transform)\n",
    "\n",
    "rows, columns = 2, 2\n",
    "n_row, n_col = 0, 0\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(20,10))\n",
    "for num_ft in num_cols_transform:\n",
    "    if n_col < columns:\n",
    "        axes[n_row][n_col].hist(df_train_trans[num_ft])\n",
    "        axes[n_row][n_col].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    else:\n",
    "        n_row = 1\n",
    "        axes[n_row][n_col-columns].hist(df_train_trans[num_ft])\n",
    "        axes[n_row][n_col-columns].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(output_distribution='uniform', random_state=42)\n",
    "df_train_trans = pd.DataFrame(quantile_transformer.fit_transform(df_train[num_cols_transform]),columns=num_cols_transform)\n",
    "\n",
    "rows, columns = 2, 2\n",
    "n_row, n_col = 0, 0\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(20,10))\n",
    "for num_ft in num_cols_transform:\n",
    "    if n_col < columns:\n",
    "        axes[n_row][n_col].hist(df_train_trans[num_ft])\n",
    "        axes[n_row][n_col].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    else:\n",
    "        n_row = 1\n",
    "        axes[n_row][n_col-columns].hist(df_train_trans[num_ft])\n",
    "        axes[n_row][n_col-columns].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = preprocessing.PowerTransformer(method='yeo-johnson')\n",
    "df_train_trans = pd.DataFrame(power_transformer.fit_transform(df_train[num_cols_transform]),columns=num_cols_transform)\n",
    "\n",
    "rows, columns = 2, 2\n",
    "n_row, n_col = 0, 0\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(20,10))\n",
    "for num_ft in num_cols_transform:\n",
    "    if n_col < columns:\n",
    "        axes[n_row][n_col].hist(df_train_trans[num_ft])\n",
    "        axes[n_row][n_col].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    else:\n",
    "        n_row = 1\n",
    "        axes[n_row][n_col-columns].hist(df_train_trans[num_ft])\n",
    "        axes[n_row][n_col-columns].set_title(num_ft)\n",
    "        n_col = n_col + 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log1p(df_train['previous']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log1p(df_train['pdays']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting down which transformations should be tried for which features:\n",
    "* **No transformation:** 'age', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'\n",
    "* **Power transformation with 'yeo-johnson':** 'duration', 'campaign'\n",
    "* **log1p transformation:** 'previous', 'pdays'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.86319Z",
     "iopub.status.busy": "2021-10-24T07:35:03.862447Z",
     "iopub.status.idle": "2021-10-24T07:35:03.868464Z",
     "shell.execute_reply": "2021-10-24T07:35:03.867619Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.863138Z"
    }
   },
   "outputs": [],
   "source": [
    "def mutual_info_subscribed_score(series):\n",
    "    return mutual_info_score(series,df_full_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:03.870746Z",
     "iopub.status.busy": "2021-10-24T07:35:03.869921Z",
     "iopub.status.idle": "2021-10-24T07:35:04.338342Z",
     "shell.execute_reply": "2021-10-24T07:35:04.337684Z",
     "shell.execute_reply.started": "2021-10-24T07:35:03.870704Z"
    }
   },
   "outputs": [],
   "source": [
    "mi = df_full_train[t_cat_cols].apply(mutual_info_subscribed_score)\n",
    "mi.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see that the features **marital, day_of_week, housing and loan** seem to not have any bearing on the outcome. Will check the score with and without these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check global subscription rate relation to subscription rate feature wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:04.341997Z",
     "iopub.status.busy": "2021-10-24T07:35:04.341743Z",
     "iopub.status.idle": "2021-10-24T07:35:04.348755Z",
     "shell.execute_reply": "2021-10-24T07:35:04.347674Z",
     "shell.execute_reply.started": "2021-10-24T07:35:04.341965Z"
    }
   },
   "outputs": [],
   "source": [
    "global_subscription_rate = round(df['y'].mean(),3)\n",
    "print(global_subscription_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:04.351046Z",
     "iopub.status.busy": "2021-10-24T07:35:04.350458Z",
     "iopub.status.idle": "2021-10-24T07:35:04.467935Z",
     "shell.execute_reply": "2021-10-24T07:35:04.467025Z",
     "shell.execute_reply.started": "2021-10-24T07:35:04.351Z"
    }
   },
   "outputs": [],
   "source": [
    "cons_df_group = pd.DataFrame()\n",
    "\n",
    "for c in t_cat_cols:\n",
    "    df_group = df_full_train.groupby(c)['y'].agg(['mean','count'])\n",
    "    df_group['diff'] = df_group['mean'] - global_subscription_rate\n",
    "    df_group['abs_diff'] = np.abs(df_group['mean'] - global_subscription_rate)\n",
    "    df_group['ratio'] = df_group['mean'] / global_subscription_rate\n",
    "    new_idx = [c+'_'+val for val in list(df_group.index.values)]\n",
    "    df_group.index = new_idx\n",
    "    cons_df_group = pd.concat([cons_df_group,df_group])\n",
    "display(cons_df_group.sort_values(by='ratio',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-relation of numerical features with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:04.469763Z",
     "iopub.status.busy": "2021-10-24T07:35:04.469499Z",
     "iopub.status.idle": "2021-10-24T07:35:04.489259Z",
     "shell.execute_reply": "2021-10-24T07:35:04.488552Z",
     "shell.execute_reply.started": "2021-10-24T07:35:04.469732Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full_train[t_num_cols].corrwith(df_full_train['y']).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see that most of the numerical features have correlation with target unlike categorical variables, where almost all categorical variables had lower mutual information score with target. The features age, duration, previous and cons.conf.idx have a postivie correlation, while campaign, pdays, emp.var.rate, cons.price.idx, euribor3m and nr.employed have negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check absolute correlations (without considering if it is postive or negative, but simply how highly they are correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:04.490964Z",
     "iopub.status.busy": "2021-10-24T07:35:04.490408Z",
     "iopub.status.idle": "2021-10-24T07:35:04.51362Z",
     "shell.execute_reply": "2021-10-24T07:35:04.512711Z",
     "shell.execute_reply.started": "2021-10-24T07:35:04.490928Z"
    }
   },
   "outputs": [],
   "source": [
    "np.abs(df_full_train[t_num_cols].corrwith(df_full_train['y'])).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** All the numerical features seem to be important for predicting the outcome. \n",
    "\n",
    "**Important Note:**\n",
    "The duration will not be considered in the model performance and the final model, based on the guidelines that were given with the dataset and mentioned below.\n",
    "- *Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at correlations of numerical feature with other numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:04.515563Z",
     "iopub.status.busy": "2021-10-24T07:35:04.515216Z",
     "iopub.status.idle": "2021-10-24T07:35:04.549511Z",
     "shell.execute_reply": "2021-10-24T07:35:04.548955Z",
     "shell.execute_reply.started": "2021-10-24T07:35:04.515508Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full_train[t_num_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:04.553985Z",
     "iopub.status.busy": "2021-10-24T07:35:04.553746Z",
     "iopub.status.idle": "2021-10-24T07:35:05.527021Z",
     "shell.execute_reply": "2021-10-24T07:35:05.526176Z",
     "shell.execute_reply.started": "2021-10-24T07:35:04.553955Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(df_full_train[t_num_cols].corr(), annot=True, fmt='.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see that some of the features have good correlation (>0.75) - like - emp.var.rate, nr.employed, euribor3m, cons.price.idx are co-related to each other. This makes sense as all these are social and economic context attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline-model'></a>\n",
    "### 4. Baseline model\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will check performance of baseline model using LogisticRegression with default parameters and evaluating using roc_auc_score (since the target variable has class imbalance). Will check the scores with and without the feature 'duration'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score with feature 'duration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:05.539551Z",
     "iopub.status.busy": "2021-10-24T07:35:05.538586Z",
     "iopub.status.idle": "2021-10-24T07:35:05.547045Z",
     "shell.execute_reply": "2021-10-24T07:35:05.546019Z",
     "shell.execute_reply.started": "2021-10-24T07:35:05.539502Z"
    }
   },
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "model = LogisticRegression(solver='liblinear',random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:05.548737Z",
     "iopub.status.busy": "2021-10-24T07:35:05.54851Z",
     "iopub.status.idle": "2021-10-24T07:35:07.401884Z",
     "shell.execute_reply": "2021-10-24T07:35:07.401061Z",
     "shell.execute_reply.started": "2021-10-24T07:35:05.548711Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_train = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(dict_train)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:07.403258Z",
     "iopub.status.busy": "2021-10-24T07:35:07.403035Z",
     "iopub.status.idle": "2021-10-24T07:35:07.782325Z",
     "shell.execute_reply": "2021-10-24T07:35:07.781357Z",
     "shell.execute_reply.started": "2021-10-24T07:35:07.403231Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_val = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(dict_val)\n",
    "y_pred = model.predict_proba(X_val)[:,1]\n",
    "this_auc = roc_auc_score(y_val,y_pred)\n",
    "print(this_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all scores into a pandas dataframe so that we can have a comparative study across experiments\n",
    "\n",
    "#Construct of this dataframe will be like below example\n",
    "#\"algo\", \"desc\", \"score\", \"diff\"\n",
    "#\"logisticregression\", \"description of the experiment\", 0.6896, 0.0243\n",
    "\n",
    "exp_columns = [\"algo\", \"desc\", \"score\", \"diff\"]\n",
    "exp_scores = pd.DataFrame(columns = exp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do one-hot encoding, train the model and evaluate model on validation data\n",
    "\n",
    "def evaluate(new_features,df_train_copy,df_val_copy,model):\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    "    model.fit(X_train_new,y_train)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    y_pred_new = model.predict_proba(X_val_new)[:,1]    \n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred_new)\n",
    "\n",
    "    return roc_auc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the feature 'duration' and checking the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:09.462714Z",
     "iopub.status.busy": "2021-10-24T07:35:09.462094Z",
     "iopub.status.idle": "2021-10-24T07:35:09.827209Z",
     "shell.execute_reply": "2021-10-24T07:35:09.826364Z",
     "shell.execute_reply.started": "2021-10-24T07:35:09.462648Z"
    }
   },
   "outputs": [],
   "source": [
    "#Experiment 0 - baseline\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "del df_train_copy['duration']\n",
    "df_val_copy = df_val.copy()\n",
    "del df_val_copy['duration']\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "baseline_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(baseline_auc)\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"baseline score. all features excluding duration\", \"score\": baseline_auc, \"diff\": 0}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We will consider this score (without the 'duration' feature) as the baseline score. We will now see how we can improve the score. Later we will try using different models and then again see for those, how we can further improve the score by hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting 'duration' feature from all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:09.829124Z",
     "iopub.status.busy": "2021-10-24T07:35:09.828552Z",
     "iopub.status.idle": "2021-10-24T07:35:09.83662Z",
     "shell.execute_reply": "2021-10-24T07:35:09.835769Z",
     "shell.execute_reply.started": "2021-10-24T07:35:09.829079Z"
    }
   },
   "outputs": [],
   "source": [
    "del df_train['duration']\n",
    "del df_full_train['duration']\n",
    "del df_val['duration']\n",
    "del df_test['duration']\n",
    "del df_test_project['duration']\n",
    "t_num_cols.remove('duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline-improvement'></a>\n",
    "### 5. Improvement over baseline\n",
    "[back to TOC](#toc)\n",
    "\n",
    "Idea here is to do several experiments using the algorithm used in baseline and find methods to improve the score. Then we will try to tune the paramaters of this model to further improve the score. After having worked with this model, we will look at other algorithms and compare the scores, then we will also tune the hyperparameters for these other models. Finally we will compare all the results to select the best model and parameters, which we will then use to train the full_train dataset and do final evaluation on the test dataset\n",
    "\n",
    "#### [5.1 Logistic Regression](#logistic-regression)\n",
    "##### [5.1.1 Experiments to improve the score using LogisticRegression](#logistic-regression-1)\n",
    "   * Perform various scaling (Standard, MinMax, use Polynomial features) of the numerical features and compare scores\n",
    "   * Check scores by dropping less important features based on EDA\n",
    "   * Check effect on score by dropping one feature at a time\n",
    "   * Check effect on score by dropping multiple features that led to increased score when dropped\n",
    "   * Look at coefficients of the model to determine less important features\n",
    "\n",
    "##### [5.1.2 Model tuning for LogisticRegression](#logistic-regression-2)\n",
    "   * Choose the best experiment and find the best hyper-parameters for the model\n",
    "\n",
    "#### [5.2 DecisionTreeClassifier](#decision-tree)\n",
    "Compare score of using DecisionTree with baseline using LogisticRegression\n",
    "##### [5.2.1 Experiments to improve the score using DecisionTreeClassifier](#decision-tree-1)\n",
    "   * Perform various scaling (Standard, MinMax, use Polynomial features) of the numerical features and compare scores\n",
    "   * Check scores by dropping less important features based on EDA\n",
    "   * Check effect on score by dropping one feature at a time\n",
    "   * Check effect on score by dropping multiple features that led to increased score when dropped\n",
    "\n",
    "##### [5.2.2 Model tuning for DecisionTreeClassifier](#ldecision-tree-2)\n",
    "   * Choose the best experiment and find the best hyper-parameters for the model\n",
    "\n",
    "#### [5.3 RandomForestClassifier](#random-forest)\n",
    "Compare score of using RandomForestClassifier with baseline using LogisticRegression\n",
    "##### [5.3.1 Experiments to improve the score using RandomForestClassifier](#random-forest-1)\n",
    "   * Perform various scaling (Standard, MinMax, use Polynomial features) of the numerical features and compare scores\n",
    "   * Check scores by dropping less important features based on EDA\n",
    "   * Check effect on score by dropping one feature at a time\n",
    "   * Check effect on score by dropping multiple features that led to increased score when dropped\n",
    "\n",
    "##### [5.3.2 Model tuning for RandomForestClassifier](#random-forest-2)\n",
    "   * Choose the best experiment and find the best hyper-parameters for the model\n",
    "\n",
    "#### [5.4 XGBoost](#xgb)\n",
    "Compare score of using XGBoost with baseline using LogisticRegression\n",
    "##### [5.4.1 Experiments to improve the score using XGBoost](#xgb-1)\n",
    "   * Perform various scaling (Standard, MinMax, use Polynomial features) of the numerical features and compare scores\n",
    "   * Check scores by dropping less important features based on EDA\n",
    "   * Check effect on score by dropping one feature at a time\n",
    "   * Check effect on score by dropping multiple features that led to increased score when dropped\n",
    "\n",
    "##### [5.4.2 Model tuning for XGBoost](#xgb-2)\n",
    "   * Choose the best experiment and find the best hyper-parameters for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic-regression'></a>\n",
    "#### 5.1 Logistic Regression (continued...)\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic-regression-1'></a>\n",
    "#### 5.1.1 Experiments to improve the score using LogisticRegression\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models work best when all the features have similar scale. Let us check whether scaling of numerical features helps increase the score. Using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 1\n",
    "\n",
    "scaler_std = preprocessing.StandardScaler()\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "# df_train_copy.reset_index(drop=True,inplace=True)\n",
    "X_train_num = scaler_std.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "# df_val_copy.reset_index(drop=True,inplace=True)\n",
    "X_val_num = scaler_std.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"standardscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** There is considerable improvement in score after scaling using StandardScaler. Let us check results with other scaling (MinMaxScaler) and preprocessing (Polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2\n",
    "\n",
    "scaler_minmax = preprocessing.MinMaxScaler()\n",
    "# df_train_copy.reset_index(drop=True,inplace=True)\n",
    "df_train_copy = df_train.copy()\n",
    "\n",
    "X_train_num = scaler_minmax.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "# df_val_copy.reset_index(drop=True,inplace=True)\n",
    "X_val_num = scaler_minmax.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"minmaxscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** There is further slight improvement in score with MinMaxScaler compared to StandarScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create polynomial features and check score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy = pd.concat([df_train_copy, df_train_poly], axis=1)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy = pd.concat([df_val_copy, df_val_poly], axis=1)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"polynomialfeatures\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Score after using polynomial features has decreased and is even less than the baseline (here we kept original features as well as the polynomial features). Let us check replacing original numerical features by the corresponding polynomil features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 4\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_train_poly.columns.values)\n",
    "df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"polynomialfeatures replacing org. features\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Using polynomial features replacing original is getting better result, however is still less than baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check scores with various transformations on numerical features that we noted down during additional EDA\n",
    "* Power transformation with 'yeo-johnson': 'duration', 'campaign' -- since we have deleted 'duration' feature, will consider only 'campaign'\n",
    "* log1p transformation: 'previous', 'pdays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5 - power transform 'campaign'\n",
    "cols_transform = ['campaign']\n",
    "power_transformer = preprocessing.PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_trans = pd.DataFrame(power_transformer.fit_transform(df_train_copy[cols_transform]),columns=cols_transform)\n",
    "df_train_copy[cols_transform] = df_train_trans[cols_transform]\n",
    "\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_trans = pd.DataFrame(power_transformer.transform(df_val_copy[cols_transform]),columns=cols_transform)\n",
    "df_val_copy[cols_transform] = df_val_trans[cols_transform]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"powertransform 'campaign'\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6 - log1p transform 'previous'\n",
    "cols_transform = ['previous']\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy[cols_transform] = np.log1p(df_train_copy[cols_transform])\n",
    "\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy[cols_transform] = np.log1p(df_val_copy[cols_transform])\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"log1p 'previous'\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 7 - log1p transform 'pdays'\n",
    "cols_transform = ['pdays']\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy[cols_transform] = np.log1p(df_train_copy[cols_transform])\n",
    "\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy[cols_transform] = np.log1p(df_val_copy[cols_transform])\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"log1p 'pdays'\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 8 - Use log1p transform for 'pdays' and for other numerical features use StandardScaler\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "cols_transform = t_num_cols.copy()\n",
    "cols_transform.remove('pdays')\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = scaler.fit_transform(df_train_copy[cols_transform])\n",
    "df_train_copy[cols_transform] = pd.DataFrame(X_train_num,columns=cols_transform)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = scaler.transform(df_val_copy[cols_transform])\n",
    "df_val_copy[cols_transform] = pd.DataFrame(X_val_num,columns=cols_transform)\n",
    "\n",
    "df_train_copy['pdays'] = np.log1p(df_train_copy['pdays'])\n",
    "df_val_copy['pdays'] = np.log1p(df_val_copy['pdays'])\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": \"log1p 'pdays' rest standardscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** From experiments so far, performing various transformation on numerical features, we can see that we got **best score with MixMaxScaler**, then log1p of 'pdays', then using StandardScaler. Since we decided not to use MinMaxScaler, and since score of simply using StandardScaler without any other transformations is equally good (very small difference), we will be choosing this method of transformation for further experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the coefficients of the trained Logistic regression model to see which are the features that do not help much (coefficient values close to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do one-hot encoding of categorical features, StandardScaler processing of numerical features, train the model and evaluate model on validation data\n",
    "\n",
    "def evaluate(new_features,df_train_copy,df_val_copy,model):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    \n",
    "    num_cols = list(df_train_copy.columns[df_train_copy.dtypes != 'object'])\n",
    "    \n",
    "    cols_transform = num_cols.copy()\n",
    "    if 'pdays' in cols_transform:\n",
    "        cols_transform.remove('pdays')\n",
    "        df_train_copy['pdays'] = np.log1p(df_train_copy['pdays'])\n",
    "        df_val_copy['pdays'] = np.log1p(df_val_copy['pdays'])\n",
    "    \n",
    "    X_train_num = scaler_minmax.fit_transform(df_train_copy[cols_transform])\n",
    "    df_train_copy[cols_transform] = pd.DataFrame(X_train_num,columns=cols_transform)\n",
    "\n",
    "    X_val_num = scaler_minmax.transform(df_val_copy[cols_transform])\n",
    "    df_val_copy[cols_transform] = pd.DataFrame(X_val_num,columns=cols_transform)\n",
    "\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    "    model.fit(X_train_new,y_train)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    y_pred_new = model.predict_proba(X_val_new)[:,1]    \n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred_new)\n",
    "\n",
    "    return roc_auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will again process the data with StandardScaler and train the model and then will check the coefficients\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = dv.get_feature_names()\n",
    "df_ = pd.DataFrame()\n",
    "df_['features'] = features_names\n",
    "df_['coef'] = model.coef_[0].round(3)\n",
    "df_['abs_coef'] = np.abs(model.coef_[0].round(3))\n",
    "df_ = df_.sort_values('abs_coef').reset_index(drop=True)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on out EDA, we had seen that the features 'loan','housing','day_of_week','marital' had very less mutual information with the target. Let us check the score of model trained without these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:11.183703Z",
     "iopub.status.busy": "2021-10-24T07:35:11.183376Z",
     "iopub.status.idle": "2021-10-24T07:35:11.467521Z",
     "shell.execute_reply": "2021-10-24T07:35:11.46658Z",
     "shell.execute_reply.started": "2021-10-24T07:35:11.183673Z"
    }
   },
   "outputs": [],
   "source": [
    "#Experiment 9\n",
    "\n",
    "drop_features = ['loan','housing','day_of_week','marital']   # Features observed to be least significant in EDA\n",
    "\n",
    "#Defining description based on features being dropped\n",
    "entry_desc = \"EDA obs deleted features 'loan','housing','day_of_week','marital'\"\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(this_auc)\n",
    "print('%.6f' % (round(this_auc - baseline_auc,6)))\n",
    "score_entry = {\"algo\": \"logisticregression\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Thus we can see that there is extremely tiny difference in the scores with and without the features 'loan','housing','day_of_week','marital'. Thus the model is confirming our observations in EDA. Also the score seems to have slightly improved after removing the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate effect on score by dropping one feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:35:11.470029Z",
     "iopub.status.busy": "2021-10-24T07:35:11.469427Z",
     "iopub.status.idle": "2021-10-24T07:35:11.488885Z",
     "shell.execute_reply": "2021-10-24T07:35:11.487895Z",
     "shell.execute_reply.started": "2021-10-24T07:35:11.469977Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T07:58:29.309471Z",
     "iopub.status.busy": "2021-10-24T07:58:29.308808Z",
     "iopub.status.idle": "2021-10-24T07:58:30.798211Z",
     "shell.execute_reply": "2021-10-24T07:58:30.796952Z",
     "shell.execute_reply.started": "2021-10-24T07:58:29.309423Z"
    }
   },
   "outputs": [],
   "source": [
    "#Score for each feature being dropped and it's difference with baseline score\n",
    "\n",
    "#Experiment 10 onwards\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "feature_roc_auc_scores = []\n",
    "\n",
    "for drop_feature in list(features_list):\n",
    "    new_features = list(features_list.drop(drop_feature))\n",
    "    this_auc = evaluate(new_features,df_train_copy,df_val_copy,model)\n",
    "    feature_roc_auc_scores.append((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc)))\n",
    "    print(str((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc))))\n",
    "    entry_desc = f\"delete one feature {drop_feature}\"\n",
    "    score_entry = {\"algo\": \"logisticregression\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check difference in scores - positive difference indicating score improved on removing the feature (feature is unnecessary and impacting model), negative difference indicating score reduced on removing the feature (feature is useful) and the magnitude of the value indicating how important (more or less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-24T07:35:12.957389Z",
     "iopub.status.idle": "2021-10-24T07:35:12.958343Z",
     "shell.execute_reply": "2021-10-24T07:35:12.958159Z",
     "shell.execute_reply.started": "2021-10-24T07:35:12.958135Z"
    }
   },
   "outputs": [],
   "source": [
    "df_feature_roc_auc = pd.DataFrame(feature_roc_auc_scores,columns=['feature','new_roc_auc_score','diff_roc_auc_score','abs_diff_roc_auc_score'])\n",
    "df_sorted = df_feature_roc_auc.sort_values(by='diff_roc_auc_score',ascending=False)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(df_sorted['feature'],df_sorted['diff_roc_auc_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see the following:\n",
    "* Deleting the feature 'month' results into reduced score\n",
    "* Deleting any other features is improving the score although not very much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which features result into least change in score than baseline, irrespective of whether the difference is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_roc_auc.sort_values(by='abs_diff_roc_auc_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Deleting any of the features has very minimal effect on score [less than 0.008 - 0.0015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will now experiment with dropping a groups of features (features that on dropping result into increased score) and see the effect on score. We will select the top 8 features that have positive effect when removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = list(df_feature_roc_auc.sort_values(by='abs_diff_roc_auc_score',ascending=False).head(8)['feature'].values)\n",
    "top_features.remove('month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#Function to get all combinations of all features\n",
    "def drop_features(features):\n",
    "    drop_features_list = []\n",
    "    for L in range(1, len(features)+1):\n",
    "        for subset in itertools.combinations(features, L):\n",
    "            drop_features_list.append(list(subset))\n",
    "    return drop_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 29 onwards\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "drop_features_list = drop_features(top_features)\n",
    "\n",
    "for drop_features in drop_features_list:\n",
    "    #Defining description based on features being dropped\n",
    "    entry_desc = f'deleted feature {str(drop_features).replace(\"[\",\"\").replace(\"]\",\"\")}'\n",
    "\n",
    "    df_train_copy = df_train.copy()\n",
    "    df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    df_val_copy = df_val.copy()\n",
    "    df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "    this_auc = evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "    print(this_auc, this_auc - baseline_auc)\n",
    "    score_entry = {\"algo\": \"logisticregression\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Removing combination of features is giving better scores that removing a single feature and almost all of these experiments are scoring very well in comparison with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)[:10]['desc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 ranking experiments are:\n",
    "* \"deleted feature 'day_of_week', 'education', 'cons.conf.idx', 'age', 'job'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'cons.conf.idx', 'age', 'job', 'previous'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'cons.conf.idx', 'job'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'cons.conf.idx', 'job', 'previous'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'age', 'job'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'job'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'job', 'previous'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'age', 'job', 'previous'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'cons.conf.idx', 'marital', 'age'\"\n",
    "* \"deleted feature 'day_of_week', 'education', 'cons.conf.idx', 'age'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic-regression-2'></a>\n",
    "##### 5.1.2 Model tuning for LogisticRegression\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning the parameters using GridsearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning parameters using the top experiment: Use MinMaxScaler to transform numerical features, Delete features 'day_of_week', 'education', 'cons.conf.idx', 'age', 'job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df_full_train_copy):\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    \n",
    "    num_cols = list(df_full_train_copy.columns[df_full_train_copy.dtypes != 'object'])\n",
    "    \n",
    "    cols_transform = num_cols.copy()\n",
    "    if 'pdays' in cols_transform:\n",
    "        cols_transform.remove('pdays')\n",
    "        df_full_train_copy['pdays'] = np.log1p(df_full_train_copy['pdays'])\n",
    "    \n",
    "    X_full_train_num = scaler.fit_transform(df_full_train_copy[cols_transform])\n",
    "    df_full_train_copy[cols_transform] = pd.DataFrame(X_full_train_num,columns=cols_transform)\n",
    "\n",
    "    dict_full_train_new = df_full_train_copy.to_dict(orient='records')\n",
    "    X_full_train_new = dv.fit_transform(dict_full_train_new)\n",
    "\n",
    "    return X_full_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best hyperparameters. \n",
    "\n",
    "#Will do this in 2 steps, since 'newton-cg' and 'lbfgs' supports 'l2' and 'none', while 'liblinear' supports 'l1' and 'l2'\n",
    "\n",
    "#Step1: Run for 'newton-cg', 'lbfgs' with 'l2' and 'none'\n",
    "\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "max_iters = np.linspace(10, 200,20)\n",
    "\n",
    "drop_features = ['day_of_week', 'education', 'cons.conf.idx', 'age', 'job']\n",
    "entry_desc = f'deleted feature {str(drop_features).replace(\"[\",\"\").replace(\"]\",\"\")}'\n",
    "  \n",
    "df_full_train_copy = df_full_train.copy()\n",
    "df_full_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "\n",
    "y_full_train = df_full_train_copy['y'].values\n",
    "del df_full_train_copy['y']\n",
    "\n",
    "# dict_full_train_dict = df_full_train_copy.to_dict(orient='records')\n",
    "# X_full_train = dv.fit_transform(dict_full_train_dict)\n",
    "X_full_train = pre_process(df_full_train_copy)\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values,max_iter=max_iters)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search_1 = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
    "grid_result_1 = grid_search_1.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result_1.cv_results_['mean_test_score']\n",
    "stds = grid_result_1.cv_results_['std_test_score']\n",
    "params = grid_result_1.cv_results_['params']\n",
    "columns = ['algo','mean_test_score','std_test_score','params']\n",
    "df_gridcv_results_1 = pd.DataFrame(columns=columns)\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    score_entry = {\"algo\": \"logisticregression\", \"mean_test_score\": mean, \"std_test_score\": stdev, \"params\": param}\n",
    "    df_gridcv_results_1 = df_gridcv_results_1.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results_1.sort_values(by='mean_test_score',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best hyperparameters. \n",
    "\n",
    "#Will do this in 2 steps, since 'newton-cg' and 'lbfgs' supports 'l2' and 'none', while 'liblinear' supports 'l1' and 'l2'\n",
    "\n",
    "#Step2: Run for 'liblinear' with 'l1' and l2'\n",
    "\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['liblinear']\n",
    "penalty = ['l1', 'l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "max_iters = np.linspace(10, 200,20)\n",
    "\n",
    "drop_features = ['day_of_week', 'education', 'cons.conf.idx', 'age', 'job']\n",
    "entry_desc = f'deleted feature {str(drop_features).replace(\"[\",\"\").replace(\"]\",\"\")}'\n",
    "  \n",
    "df_full_train_copy = df_full_train.copy()\n",
    "df_full_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "\n",
    "y_full_train = df_full_train_copy['y'].values\n",
    "del df_full_train_copy['y']\n",
    "\n",
    "# dict_full_train_dict = df_full_train_copy.to_dict(orient='records')\n",
    "# X_full_train = dv.fit_transform(dict_full_train_dict)\n",
    "X_full_train = pre_process(df_full_train_copy)\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values,max_iter=max_iters)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search_2 = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
    "grid_result_2 = grid_search_1.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result_2.cv_results_['mean_test_score']\n",
    "stds = grid_result_2.cv_results_['std_test_score']\n",
    "params = grid_result_2.cv_results_['params']\n",
    "columns = ['algo','mean_test_score','std_test_score','params']\n",
    "df_gridcv_results_2 = pd.DataFrame(columns=columns)\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    score_entry = {\"algo\": \"logisticregression\", \"mean_test_score\": mean, \"std_test_score\": stdev, \"params\": param}\n",
    "    df_gridcv_results_2 = df_gridcv_results_2.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results_2.sort_values(by='mean_test_score',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results_2.sort_values(by='mean_test_score',ascending=False).head(30)['params'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results = pd.concat([df_gridcv_results_1, df_gridcv_results_2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results_2.sort_values(by='mean_test_score',ascending=False).head(30)['params'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results.sort_values(by='mean_test_score',ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from hyper-parameter tuning concatenated from the 2 steps\n",
    "# df_gridcv_results.to_csv('project-logistic-regression-gridcv-scores-29oct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decision-tree'></a>\n",
    "#### 5.2 DecisionTreeClassifier\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do one-hot encoding, train the model and evaluate model on validation data\n",
    "\n",
    "def dt_evaluate(new_features,df_train_copy,df_val_copy,model):\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    "    model.fit(X_train_new,y_train)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    y_pred_new = model.predict_proba(X_val_new)[:,1]    \n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred_new)\n",
    "\n",
    "    #Returning dv also, so that we can use this to check decision made by decisiontree\n",
    "    return roc_auc_val, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#Baseline score with DecisionTree\n",
    "\n",
    "#Defining description based on features being dropped\n",
    "entry_desc = f'baseline score. all features except duration'\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(this_auc, this_auc - baseline_auc)\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** With DecisionTree the score is worse than LogisticRegression - this might be due to overfitting. To check overfitting, we will check the score on y_train itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "dict_train_new = df_train_copy[features_list].to_dict(orient='records')\n",
    "X_train_new = dv.fit_transform(dict_train_new)\n",
    "model.fit(X_train_new,y_train)\n",
    "\n",
    "y_pred_train = model.predict_proba(X_train_new)[:,1]    \n",
    "this_auc = roc_auc_score(y_train,y_pred_train)\n",
    "\n",
    "print(this_auc)\n",
    "print('%.6f' % (round(baseline_auc - this_auc,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Indeed there is overfitting and hence we did not get good score in previous experiment. This must be due to max_depth default being infinite which causes DecisionTree to almost memorize training data and hence overfitting. This is the same reason why the baseline score with DecisionTree was also bad.\n",
    "\n",
    "Let us set max_depth to 4 and take a new baseline. For further experiments we can then keep this same value for max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with DecisionTree with max_depth = 4.\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42,max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New baseline score with DecisionTree with max_depth = 4. Experiment 2\n",
    "entry_desc = f'new baseline max_depth=4. all features except duration'\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(this_auc, this_auc - baseline_auc)\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Now we have comparatively better score but it is slightly less than the logisticregression baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how decisiontree made its decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "print(export_text(model,feature_names=dv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decision-tree-1'></a>\n",
    "##### 5.2.1 Experiments to improve the score using DecisionTreeClassifier\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on out EDA, we had seen that the features 'loan','housing','day_of_week','marital' had very less mutual information with the target. Let us check the score of model trained without these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3\n",
    "\n",
    "drop_features = ['loan','housing','day_of_week','marital']   # Features observed to be least significant in EDA\n",
    "\n",
    "#Defining description based on features being dropped\n",
    "entry_desc = \"EDA obs deleted features 'loan','housing','day_of_week','marital'\"\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(this_auc)\n",
    "print('%.6f' % (round(baseline_auc - this_auc,6)))\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** With DecisionTree the difference in the scores with and without the features 'loan', 'housing', 'day_of_week', 'marital' is very less, thus confirming our observations in EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate scores by transforming the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 4\n",
    "\n",
    "scaler_std = preprocessing.StandardScaler()\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "# df_train_copy.reset_index(drop=True,inplace=True)\n",
    "X_train_num = scaler_std.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "# df_val_copy.reset_index(drop=True,inplace=True)\n",
    "X_val_num = scaler_std.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": \"standardscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5\n",
    "\n",
    "scaler_minmax = preprocessing.MinMaxScaler()\n",
    "# df_train_copy.reset_index(drop=True,inplace=True)\n",
    "df_train_copy = df_train.copy()\n",
    "\n",
    "X_train_num = scaler_minmax.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "# df_val_copy.reset_index(drop=True,inplace=True)\n",
    "X_val_num = scaler_minmax.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": \"minmaxscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy = pd.concat([df_train_copy, df_train_poly], axis=1)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy = pd.concat([df_val_copy, df_val_poly], axis=1)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": \"polynomialfeatures\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 7\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_train_poly.columns.values)\n",
    "df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": \"polynomialfeatures replacing org. features\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 8 - power transform 'campaign'\n",
    "cols_transform = ['campaign']\n",
    "power_transformer = preprocessing.PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_trans = pd.DataFrame(power_transformer.fit_transform(df_train_copy[cols_transform]),columns=cols_transform)\n",
    "df_train_copy[cols_transform] = df_train_trans[cols_transform]\n",
    "\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_trans = pd.DataFrame(power_transformer.transform(df_val_copy[cols_transform]),columns=cols_transform)\n",
    "df_val_copy[cols_transform] = df_val_trans[cols_transform]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"decisiontree\", \"desc\": \"powertransform 'campaign'\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** From the experiments so far, we can see that only the score with polynomial features is better than the baseline (new baseline of decisiontree). Also, since the score with added polynomial features and replacing numerical features with polynomial features is the same, we will choose replacing option, as it will reduce number of features to be trained on. We will use this method for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to do one-hot encoding, replace numerical features with polynomial features, train the model and evaluate model on validation data\n",
    "\n",
    "def dt_evaluate(new_features,df_train_copy,df_val_copy,model):\n",
    "    poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "    df_train_copy = df_train_copy[new_features]\n",
    "    df_val_copy = df_val_copy[new_features]\n",
    "    \n",
    "    num_cols = list(df_train_copy.columns[df_train_copy.dtypes != 'object'])\n",
    "    X_train_num = poly.fit_transform(df_train_copy[num_cols])\n",
    "    df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "    df_train_copy.drop(num_cols,axis=1,inplace=True)\n",
    "    poly_cols = list(df_train_poly.columns.values)\n",
    "    df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "    X_val_num = poly.transform(df_val_copy[num_cols])\n",
    "    df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "    df_val_copy.drop(num_cols,axis=1,inplace=True)\n",
    "    df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "    new_features = list(df_train_copy.columns.values)\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    "    model.fit(X_train_new,y_train)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    y_pred_new = model.predict_proba(X_val_new)[:,1]    \n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred_new)\n",
    "\n",
    "    #Returning dv also, so that we can use this to check decision made by decisiontree\n",
    "    return roc_auc_val, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate effect on score by dropping one feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score for each feature being dropped and it's difference with baseline score\n",
    "\n",
    "#Experiment 9 onwards\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "feature_roc_auc_scores = []\n",
    "\n",
    "for drop_feature in list(features_list):\n",
    "    new_features = list(features_list.drop(drop_feature))\n",
    "    this_auc, dv = dt_evaluate(new_features,df_train_copy,df_val_copy,model)\n",
    "    feature_roc_auc_scores.append((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc)))\n",
    "    print(str((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc))))\n",
    "    entry_desc = f\"delete one feature {drop_feature}\"\n",
    "    score_entry = {\"algo\": \"decisiontree\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_roc_auc = pd.DataFrame(feature_roc_auc_scores,columns=['feature','new_roc_auc_score','diff_roc_auc_score','abs_diff_roc_auc_score'])\n",
    "df_sorted = df_feature_roc_auc.sort_values(by='diff_roc_auc_score',ascending=False)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(df_sorted['feature'],df_sorted['diff_roc_auc_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Score is higher when dropping any of the feature (one feature at a time), except for 'contact', 'pdays', where score is decreasing when feature is dropped. Lets us check these scores in comparison with score when using polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when compared to base score with polynomial features, score is higher only when dropping any of 'month', 'campaign', 'cons.conf.idx', 'age', 'previous'. Let us check if dropping any combination of these features further increases the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#Function to get all combinations of all features\n",
    "def drop_features(features):\n",
    "    drop_features_list = []\n",
    "    for L in range(2, len(features)+1):\n",
    "        for subset in itertools.combinations(features, L):\n",
    "            drop_features_list.append(list(subset))\n",
    "    return drop_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['month', 'campaign', 'cons.conf.idx', 'age', 'previous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 28 onwards\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "drop_features_list = drop_features(top_features)\n",
    "\n",
    "for drop_features in drop_features_list:\n",
    "    #Defining description based on features being dropped\n",
    "    entry_desc = f'deleted feature {str(drop_features).replace(\"[\",\"\").replace(\"]\",\"\")}'\n",
    "\n",
    "    df_train_copy = df_train.copy()\n",
    "    df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    df_val_copy = df_val.copy()\n",
    "    df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "    this_auc, dv = dt_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "    print(this_auc, this_auc - baseline_auc)\n",
    "    score_entry = {\"algo\": \"decisiontree\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Deleting multiple features among ['month', 'campaign', 'cons.conf.idx', 'age', 'previous'] has increased the score with good positive difference. Will choose the combination that led to highest score, and then perform parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(1)['desc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Deleting all the features 'month', 'campaign', 'cons.conf.idx', 'age', 'previous' has led to the best score. We will now do parameter tuning and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decision-tree-2'></a>\n",
    "##### 5.2.2 Model tuning for DecisionTreeClassifier\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters using GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "model = DecisionTreeClassifier()\n",
    "max_depths = [1, 2, 3, 4, 5, 6, 10, 15, 20, None]\n",
    "min_samples_leafs = [1, 2, 5, 10, 15, 20, 100, 200, 500]\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "criteria = [\"gini\", \"entropy\"]\n",
    "\n",
    "drop_features = ['month', 'campaign', 'cons.conf.idx', 'age', 'previous']\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "\n",
    "dict_train_new = df_train_copy.to_dict(orient='records')\n",
    "X_train_new = dv.fit_transform(dict_train_new)\n",
    "\n",
    "\n",
    "# define grid search\n",
    "grid = dict(max_depth=max_depths,min_samples_leaf=min_samples_leafs,max_features=max_features,criterion=criteria)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
    "grid_result = grid_search.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "columns = ['algo','mean_test_score','std_test_score','params']\n",
    "df_gridcv_results = pd.DataFrame(columns=columns)\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    score_entry = {\"algo\": \"decisiontree\", \"mean_test_score\": mean, \"std_test_score\": stdev, \"params\": param}\n",
    "    df_gridcv_results = df_gridcv_results.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results.sort_values(by='mean_test_score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gridcv_results.to_csv('project-decisiontree-gridcv-scores-29oct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random-forest'></a>\n",
    "#### 5.3 RandomForestClassifier\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do one-hot encoding, train the model and evaluate model on validation data\n",
    "\n",
    "def rf_evaluate(new_features,df_train_copy,df_val_copy,model):\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    "    model.fit(X_train_new,y_train)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    y_pred_new = model.predict_proba(X_val_new)[:,1]    \n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred_new)\n",
    "\n",
    "    #Returning dv als, so that we can use this to check decision made by decisiontree\n",
    "    return roc_auc_val, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with RandomForestClassifier.\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated baseline score with DecisionTree with max_depth = 4 and no one-hot encoding. Experiment 2\n",
    "entry_desc = f'baseline score. all features except duration'\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(this_auc, this_auc - baseline_auc)\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** The baseline score using RandomForest is slightly lesser than baseline using LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random-forest-1'></a>\n",
    "##### 5.3.1 Experiments to improve the score using RandomForestClassifier\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2\n",
    "\n",
    "drop_features = ['loan','housing','day_of_week','marital']   # Features observed to be least significant in EDA\n",
    "\n",
    "#Defining description based on features being dropped\n",
    "entry_desc = \"EDA obs deleted features 'loan','housing','day_of_week','marital'\"\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "print(this_auc)\n",
    "print('%.6f' % (round(baseline_auc - this_auc,6)))\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** After deleting least important features as per EDA, with Randomforest, there is a slight impact on score compared to baseline (as compared to minimal impact when using LogisticRegression or DecisionTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate scores by transforming the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3\n",
    "\n",
    "scaler_std = preprocessing.StandardScaler()\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = scaler_std.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = scaler_std.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": \"standardscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 4\n",
    "\n",
    "scaler_minmax = preprocessing.MinMaxScaler()\n",
    "# df_train_copy.reset_index(drop=True,inplace=True)\n",
    "df_train_copy = df_train.copy()\n",
    "\n",
    "X_train_num = scaler_minmax.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "# df_val_copy.reset_index(drop=True,inplace=True)\n",
    "X_val_num = scaler_minmax.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": \"minmaxscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy = pd.concat([df_train_copy, df_train_poly], axis=1)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy = pd.concat([df_val_copy, df_val_poly], axis=1)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": \"polynomialfeatures\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_train_poly.columns.values)\n",
    "df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": \"polynomialfeatures replacing org. features\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 7 - power transform 'campaign'\n",
    "cols_transform = ['campaign']\n",
    "power_transformer = preprocessing.PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_trans = pd.DataFrame(power_transformer.fit_transform(df_train_copy[cols_transform]),columns=cols_transform)\n",
    "df_train_copy[cols_transform] = df_train_trans[cols_transform]\n",
    "\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_trans = pd.DataFrame(power_transformer.transform(df_val_copy[cols_transform]),columns=cols_transform)\n",
    "df_val_copy[cols_transform] = df_val_trans[cols_transform]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "\n",
    "score_entry = {\"algo\": \"randomforest\", \"desc\": \"powertransform 'campaign'\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** From the experiments so far, we can see that using polynomial features increased the score in comparison with baseline of randomforest, although is still less than the main baseline score (with logistic regression). Also other transformations led to decreased score. Thus we will use polynomial features for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to do one-hot encoding, replace numerical features with polynomial features, train the model and evaluate model on validation data\n",
    "\n",
    "def rf_evaluate(new_features,df_train_copy,df_val_copy,model):\n",
    "    poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "    df_train_copy = df_train_copy[new_features]\n",
    "    df_val_copy = df_val_copy[new_features]\n",
    "    \n",
    "    num_cols = list(df_train_copy.columns[df_train_copy.dtypes != 'object'])\n",
    "    X_train_num = poly.fit_transform(df_train_copy[num_cols])\n",
    "    df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "    df_train_copy.drop(num_cols,axis=1,inplace=True)\n",
    "    poly_cols = list(df_train_poly.columns.values)\n",
    "    df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "    X_val_num = poly.transform(df_val_copy[num_cols])\n",
    "    df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "    df_val_copy.drop(num_cols,axis=1,inplace=True)\n",
    "    df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "    new_features = list(df_train_copy.columns.values)\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    "    model.fit(X_train_new,y_train)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    y_pred_new = model.predict_proba(X_val_new)[:,1]    \n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred_new)\n",
    "\n",
    "    #Returning dv also, so that we can use this to check decision made by randomforest\n",
    "    return roc_auc_val, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate effect on score by dropping one feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score for each feature being dropped and it's difference with baseline score\n",
    "\n",
    "#Experiment 8 onwards\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "feature_roc_auc_scores = []\n",
    "\n",
    "for drop_feature in list(features_list):\n",
    "    new_features = list(features_list.drop(drop_feature))\n",
    "    this_auc, dv = rf_evaluate(new_features,df_train_copy,df_val_copy,model)\n",
    "    feature_roc_auc_scores.append((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc)))\n",
    "    print(str((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc))))\n",
    "    entry_desc = f\"delete one feature {drop_feature}\"\n",
    "    score_entry = {\"algo\": \"randomforest\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** When compared with polynomial features using randomforest:\n",
    "\n",
    "deleting one feature has positive impact (increased score) for features - 'cons.price.idx', 'emp.var.rate', 'cons.conf.idx', 'housing', 'previous', 'nr.employed', 'month', 'pdays', 'education', 'poutcome'\n",
    "\n",
    "while deleting one feature has negative impact (reduced score) for features - 'default', 'day_of_week', 'loan', 'marital', 'contact', 'age', 'campaign', 'job', 'euribor3m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us experiment with removing a group of features that affected positively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#Function to get all combinations of all features\n",
    "def drop_features(features):\n",
    "    drop_features_list = []\n",
    "    for L in range(2, len(features)+1):\n",
    "        for subset in itertools.combinations(features, L):\n",
    "            drop_features_list.append(list(subset))\n",
    "    return drop_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['cons.price.idx', 'emp.var.rate', 'cons.conf.idx', 'housing', 'previous', 'nr.employed', 'month', 'pdays', 'education', 'poutcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 27 onwards\n",
    "\n",
    "#Testing combination of multiple feature drops\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "drop_features_list = drop_features(top_features)\n",
    "\n",
    "for drop_features in drop_features_list:\n",
    "    #Defining description based on features being dropped\n",
    "    entry_desc = f'deleted feature {str(drop_features).replace(\"[\",\"\").replace(\"]\",\"\")}'\n",
    "\n",
    "    df_train_copy = df_train.copy()\n",
    "    df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    df_val_copy = df_val.copy()\n",
    "    df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "    this_auc, dv = rf_evaluate(features_list,df_train_copy,df_val_copy,model)\n",
    "    print(this_auc, this_auc - baseline_auc)\n",
    "    score_entry = {\"algo\": \"randomforest\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_scores.to_csv('rf_exp_scores-29oct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Dropping multiple features that had positive impact is improving the score for several combinations. Will choose the top scoring combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(1)['desc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will tune parameters and check the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random-forest-2'></a>\n",
    "##### 5.3.2 Model tuning for RandomForestClassifier\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "n_estimators = range(10, 120, 10)\n",
    "min_samples_leafs = [5, 10, 15, 20]\n",
    "max_features = [\"auto\", \"sqrt\", None]\n",
    "max_depths = [2, 3, 4, 5, 7, 10, 15, 30]\n",
    "warm_start = [True]\n",
    "random_state = [42]\n",
    "n_jobs = [-1]\n",
    "\n",
    "\n",
    "drop_features = ['cons.price.idx', 'emp.var.rate', 'cons.conf.idx', 'housing', 'nr.employed', 'pdays', 'education']\n",
    "\n",
    "# define grid search\n",
    "random_grid = {\n",
    "                'n_estimators': n_estimators,\n",
    "                'min_samples_leaf': min_samples_leafs,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depths,\n",
    "                'warm_start': warm_start,\n",
    "                'random_state': random_state,\n",
    "                'n_jobs': n_jobs,\n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "df_full_train_copy = df_full_train.copy()\n",
    "df_full_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "y_full_train = df_full_train_copy['y']\n",
    "del df_full_train_copy['y']\n",
    "\n",
    "dict_full_train_new = df_full_train_copy.to_dict(orient='records')\n",
    "X_full_train_new = dv.fit_transform(dict_full_train_new)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(model, param_grid=random_grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0,verbose=1)\n",
    "grid_result = grid_search.fit(X_full_train_new, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "columns = ['algo','mean_test_score','std_test_score','params']\n",
    "df_gridcv_results = pd.DataFrame(columns=columns)\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    score_entry = {\"algo\": \"randomforest\", \"mean_test_score\": mean, \"std_test_score\": stdev, \"params\": param}\n",
    "    df_gridcv_results = df_gridcv_results.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results.to_csv('project-randomforest-gridcv-scores-30oct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcv_results.sort_values(by='mean_test_score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb'></a>\n",
    "#### 5.4 XGBoost\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "feature_names = dv.get_feature_names()\n",
    "dtrain = xgb.DMatrix(X_train,label=y_train,feature_names=feature_names)\n",
    "\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "dval = xgb.DMatrix(X_val,label=y_val,feature_names=feature_names)\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "model = xgb.train(xgb_params,dtrain)\n",
    "\n",
    "y_pred = model.predict(dval)\n",
    "this_auc = roc_auc_score(y_val,y_pred)\n",
    "this_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_desc = 'baseline xgb'\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Score using XGBoost without any feature engineering or parameter tuning is better than the baseline. Let us check how the score improves / degrades with higher number of rounds of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xgb_output(output):\n",
    "    results = []\n",
    "\n",
    "    for line in output.stdout.strip().split('\\n'):\n",
    "        it_line, train_line, val_line = line.split('\\t')\n",
    "\n",
    "        it = int(it_line.strip('[]'))\n",
    "        train = float(train_line.split(':')[1])\n",
    "        val = float(val_line.split(':')[1])\n",
    "\n",
    "        results.append((it, train, val))\n",
    "    \n",
    "    columns = ['num_iter', 'train_auc', 'val_auc']\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "#Experiment 2\n",
    "\n",
    "watchlist = [(dtrain,'train'),(dval,'val')]\n",
    "\n",
    "model = xgb.train(xgb_params,dtrain,num_boost_round=200,evals=watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = parse_xgb_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_score.num_iter, df_score.train_auc, label='train')\n",
    "plt.plot(df_score.num_iter, df_score.val_auc, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while score on validation data improves a bit initially, somewhere from the initial rounds onwards it starts decreasing while score on training continues to increase as number of rounds are increased (this is overfitting). Let us check till what number of rounds is the score actually increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_score.num_iter, df_score.val_auc, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.sort_values(by='val_auc',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see that around num_boost_round 3 the score is highest and then it starts decreasing (due to possible overfitting). We will use num_boost_round=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 continued ..\n",
    "\n",
    "model = xgb.train(xgb_params,dtrain,num_boost_round=3)\n",
    "\n",
    "y_pred = model.predict(dval)\n",
    "this_auc = roc_auc_score(y_val,y_pred)\n",
    "this_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_desc = 'num_boost_round=3'\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** Limiting num_boost_round to 3 has increased the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb-1'></a>\n",
    "##### 5.4.1 Experiments to improve the score using XGBoost\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on out EDA, we had seen that the features 'loan','housing','day_of_week','marital' had very less mutual information with the target. Let us check the score of model trained without these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to do one-hot encoding, train the model and evaluate model on validation data.\n",
    "\n",
    "#num_boost_round=3\n",
    "\n",
    "def xgb_evaluate(new_features,df_train_copy,df_val_copy,xgb_params):\n",
    "    df_train_copy = df_train_copy[new_features]\n",
    "    df_val_copy = df_val_copy[new_features]\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "\n",
    "    train_dict = df_train_copy.to_dict(orient='records')\n",
    "    X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "    feature_names = dv.get_feature_names()\n",
    "    dtrain = xgb.DMatrix(X_train,label=y_train,feature_names=feature_names)\n",
    "\n",
    "    val_dict = df_val_copy.to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dict)\n",
    "    dval = xgb.DMatrix(X_val,label=y_val,feature_names=feature_names)\n",
    "\n",
    "    model = xgb.train(xgb_params,dtrain,num_boost_round=3)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred)\n",
    "\n",
    "    #Returning dv also, so that we can use this to check decision made by xgboost\n",
    "    return roc_auc_val, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3\n",
    "\n",
    "drop_features = ['loan','housing','day_of_week','marital']   # Features observed to be least significant in EDA\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "#Defining description based on features being dropped\n",
    "entry_desc = \"EDA obs deleted features 'loan','housing','day_of_week','marital'\"\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "print(this_auc)\n",
    "print('%.6f' % (round(baseline_auc - this_auc,6)))\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in score is small after removing the least important features as per EDA, which confirms our EDA observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate scores by transforming the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 4\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "scaler_std = preprocessing.StandardScaler()\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = scaler_std.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = scaler_std.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": \"standardscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "scaler_minmax = preprocessing.MinMaxScaler()\n",
    "df_train_copy = df_train.copy()\n",
    "\n",
    "X_train_num = scaler_minmax.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_copy[t_num_cols] = pd.DataFrame(X_train_num,columns=t_num_cols)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = scaler_minmax.transform(df_val_copy[t_num_cols])\n",
    "df_val_copy[t_num_cols] = pd.DataFrame(X_val_num,columns=t_num_cols)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": \"minmaxscaler\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy = pd.concat([df_train_copy, df_train_poly], axis=1)\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy = pd.concat([df_val_copy, df_val_poly], axis=1)\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": \"polynomialfeatures\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 7\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "X_train_num = poly.fit_transform(df_train_copy[t_num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_train_poly.columns.values)\n",
    "df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "X_val_num = poly.fit_transform(df_val_copy[t_num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy.drop(t_num_cols,axis=1,inplace=True)\n",
    "df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": \"polynomialfeatures replacing org. features\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 8 - power transform 'campaign'\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "cols_transform = ['campaign']\n",
    "power_transformer = preprocessing.PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_trans = pd.DataFrame(power_transformer.fit_transform(df_train_copy[cols_transform]),columns=cols_transform)\n",
    "df_train_copy[cols_transform] = df_train_trans[cols_transform]\n",
    "\n",
    "\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_trans = pd.DataFrame(power_transformer.transform(df_val_copy[cols_transform]),columns=cols_transform)\n",
    "df_val_copy[cols_transform] = df_val_trans[cols_transform]\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "\n",
    "score_entry = {\"algo\": \"xgb\", \"desc\": \"powertransform 'campaign'\", \"score\": this_auc, \"diff\": this_auc-baseline_auc}\n",
    "exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** From the experiments so far, we can see that using polynomial features increased the score in comparison with baseline of xgb, however all the xgb experiments have score better than baseline with logistic regression. Other transformations led to decreased score. Thus we will use polynomial features for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to do one-hot encoding, replace numerical features with polynomial features, train the model and evaluate model on validation data\n",
    "\n",
    "def xgb_evaluate(new_features,df_train_copy,df_val_copy,xgb_params):\n",
    "\n",
    "    xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "    poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "    df_train_copy = df_train_copy[new_features]\n",
    "    df_val_copy = df_val_copy[new_features]\n",
    "    \n",
    "    num_cols = list(df_train_copy.columns[df_train_copy.dtypes != 'object'])\n",
    "    X_train_num = poly.fit_transform(df_train_copy[num_cols])\n",
    "    df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "    df_train_copy.drop(num_cols,axis=1,inplace=True)\n",
    "    poly_cols = list(df_train_poly.columns.values)\n",
    "    df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "    X_val_num = poly.transform(df_val_copy[num_cols])\n",
    "    df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "    df_val_copy.drop(num_cols,axis=1,inplace=True)\n",
    "    df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "    new_features = list(df_train_copy.columns.values)\n",
    "    dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(dict_train_new)\n",
    " \n",
    "    feature_names = dv.get_feature_names()\n",
    "    dtrain = xgb.DMatrix(X_train_new,label=y_train,feature_names=feature_names)\n",
    "    \n",
    "    dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "    X_val_new = dv.transform(dict_val_new)\n",
    "    dval = xgb.DMatrix(X_val_new,label=y_val,feature_names=feature_names)\n",
    "\n",
    "    model = xgb.train(xgb_params,dtrain,num_boost_round=3)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    roc_auc_val = roc_auc_score(y_val,y_pred)\n",
    "\n",
    "    #Returning dv also, so that we can use this to check decision made by xgboost\n",
    "    return roc_auc_val, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate effect on score by dropping one feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score for each feature being dropped and it's difference with baseline score\n",
    "\n",
    "#Experiment 9 onwards\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "features_list = df_train_copy.columns\n",
    "feature_roc_auc_scores = []\n",
    "\n",
    "for drop_feature in list(features_list):\n",
    "    new_features = list(features_list.drop(drop_feature))\n",
    "    this_auc, dv = xgb_evaluate(new_features,df_train_copy,df_val_copy,xgb_params)\n",
    "    feature_roc_auc_scores.append((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc)))\n",
    "    print(str((drop_feature,this_auc,this_auc-baseline_auc,np.abs(this_auc-baseline_auc))))\n",
    "    entry_desc = f\"delete one feature {drop_feature}\"\n",
    "    score_entry = {\"algo\": \"xgb\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_roc_auc = pd.DataFrame(feature_roc_auc_scores,columns=['feature','new_roc_auc_score','diff_roc_auc_score','abs_diff_roc_auc_score'])\n",
    "df_sorted = df_feature_roc_auc.sort_values(by='diff_roc_auc_score',ascending=False)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(df_sorted['feature'],df_sorted['diff_roc_auc_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Score is higher when dropping any of the feature (one feature at a time). Lets us check these scores in comparison with score when using polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We can see that when compared to base score with polynomial features, score is higher only when dropping any of 'job', 'pdays', 'education'. Let us check if dropping any combination of these features further increases the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#Function to get all combinations of all features\n",
    "def drop_features(features):\n",
    "    drop_features_list = []\n",
    "    for L in range(2, len(features)+1):\n",
    "        for subset in itertools.combinations(features, L):\n",
    "            drop_features_list.append(list(subset))\n",
    "    return drop_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['job', 'pdays', 'education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 28 onwards\n",
    "\n",
    "xgb_params = {'seed': 42, 'eval_metric': 'auc', 'n_jobs': -1}\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_val_copy = df_val.copy()\n",
    "\n",
    "drop_features_list = drop_features(top_features)\n",
    "\n",
    "for drop_features in drop_features_list:\n",
    "    #Defining description based on features being dropped\n",
    "    entry_desc = f'deleted feature {str(drop_features).replace(\"[\",\"\").replace(\"]\",\"\")}'\n",
    "\n",
    "    df_train_copy = df_train.copy()\n",
    "    df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    df_val_copy = df_val.copy()\n",
    "    df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "    features_list = list(df_train_copy.columns.values)\n",
    "\n",
    "    this_auc, dv = xgb_evaluate(features_list,df_train_copy,df_val_copy,xgb_params)\n",
    "    print(this_auc, this_auc - baseline_auc)\n",
    "    score_entry = {\"algo\": \"xgb\", \"desc\": entry_desc, \"score\": this_auc, \"diff\": this_auc - baseline_auc}\n",
    "    exp_scores = exp_scores.append(score_entry,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores.sort_values(by='score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Dropping 'job' or 'pdays' or 'education' or 'job', 'pdays' or 'pdays', 'education' have better scores, with dropping only 'job' having the highest score. The scores are almost similar for all these top experiments, thus we can choose to use any of these. Will choose dropping 'job' and perform parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgb-2'></a>\n",
    "##### 5.4.2 Model tuning for XGBoost\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful references\n",
    "* https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "* https://www.kaggle.com/phunter/xgboost-with-gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(parameters,X_full_train_new,y_full_train):\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    grid_search = GridSearchCV(model, param_grid=parameters, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
    "    grid_result = grid_search.fit(X_full_train_new, y_full_train)\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    columns = ['algo','mean_test_score','std_test_score','params']\n",
    "    df_gridcv_results = pd.DataFrame(columns=columns)\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        score_entry = {\"algo\": \"xgb\", \"mean_test_score\": mean, \"std_test_score\": stdev, \"params\": param}\n",
    "        df_gridcv_results = df_gridcv_results.append(score_entry,ignore_index=True)\n",
    "        \n",
    "    return df_gridcv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing evaluation of best parameters for XGB using below parameters was taking a very long time using CPUs [even AWS instance with 36 cores - c4.8xlarge - was taking more than 2/3 hours].\n",
    "#Hence used Kaggle with GPU and hence some parameters like tree_method, gpu_id, predictor configured to use GPU.\n",
    "\n",
    "# define models and parameters\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "drop_features = ['job']\n",
    "\n",
    "boosters = ['gbtree']\n",
    "eval_metric = ['auc']\n",
    "objectives = ['binary:logistic']\n",
    "random_state = [42]\n",
    "\n",
    "tree_method = ['gpu_hist']\n",
    "gpu_id = [0]\n",
    "predictor = ['gpu_predictor']\n",
    "\n",
    "max_depths = [3, 4, 5, 6]\n",
    "min_child_weights = [4, 5, 6, 8, 10]\n",
    "colsample_bytrees = [0.4, 0.6, 0.8]\n",
    "\n",
    "\n",
    "# define grid search\n",
    "parameters = {\n",
    "                'random_state': random_state,\n",
    "                'eval_metric': eval_metric,\n",
    "                'booster': boosters,\n",
    "                'objective': objectives,\n",
    "                'tree_method': tree_method,\n",
    "                'gpu_id': gpu_id,\n",
    "                'predictor': predictor,\n",
    "                'max_depth': max_depths,\n",
    "                'min_child_weight': min_child_weights,\n",
    "                'colsample_bytree': colsample_bytrees,\n",
    "              }\n",
    "\n",
    "df_full_train_copy = df_full_train.copy()\n",
    "df_full_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "y_full_train = df_full_train_copy['y']\n",
    "del df_full_train_copy['y']\n",
    "\n",
    "dict_full_train_new = df_full_train_copy.to_dict(orient='records')\n",
    "X_full_train_new = dv.fit_transform(dict_full_train_new)\n",
    "\n",
    "learning_params = [\n",
    "    (1.0, 500),\n",
    "    (0.3, 800),\n",
    "    (0.1, 1000),\n",
    "    (0.05, 1500),\n",
    "    (0.01, 3000),\n",
    "]\n",
    "\n",
    "\n",
    "all_gridcv_results = pd.DataFrame()\n",
    "\n",
    "for learning_rate, n_estimators in learning_params:\n",
    "    print(learning_rate, n_estimators)\n",
    "    parameters['learning_rate'] = [learning_rate]\n",
    "    parameters['n_estimators'] = [n_estimators]\n",
    "    df_gridcv_results = eval_model(parameters,X_full_train_new,y_full_train)\n",
    "    all_gridcv_results = pd.concat([all_gridcv_results,df_gridcv_results],axis=0,ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gridcv_results.to_csv('xgb-gridcv-results-30oct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final-model'></a>\n",
    "### 6. Final model\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='choose-final-model'></a>\n",
    "#### 6.1 Compare results from hyper-parameter tuning for the different models and choose final model\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'work-dump/project-xgb-gridcv-scores-30oct.csv',\n",
    "    'work-dump/project-randomforest-gridcv-scores-30oct.csv',\n",
    "    'work-dump/project-decisiontree-gridcv-scores-29oct.csv',\n",
    "    'work-dump/project-logistic-regression-gridcv-scores-29oct.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cons_scores = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df_tmp = pd.read_csv(file)\n",
    "    df_cons_scores = pd.concat([df_cons_scores,df_tmp],axis=0,ignore_index=True)\n",
    "del df_cons_scores['Unnamed: 0']\n",
    "del df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cons_scores['algo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find top 4 scores of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cons_scores.sort_values(by='mean_test_score',ascending=False).groupby(['algo']).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cons_scores.sort_values(by='mean_test_score',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_cons_scores['std_test_score'],df_cons_scores['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Plotly since it provides interactive graphs - so that we can see what the values for the points of our interest are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_cons_scores, x=\"std_test_score\", y=\"mean_test_score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Plotly graphs appear blank when viewing previously run notebook\n",
    "\n",
    "**Observations:** We can see that 'mean_test_score' of 0.803311 has 'std_test_score' of 0.005255258 seems to be the overall best score with least std. deviation. So we will check which algorithm and what parameters got this score. From the the df_cons_scores sorted above, we can see that it is the 2nd entry from top (index 241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cons_scores.iloc[241].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, XGBoostClassifier with parameters ['booster': 'gbtree', 'colsample_bytree': 0.4, 'eval_metric': 'auc',  'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 3000, 'objective': 'binary:logistic', 'random_state': 42] got us the best score. Now we will train our final model using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xgb_output(output):\n",
    "    results = []\n",
    "\n",
    "    for line in output.stdout.strip().split('\\n'):\n",
    "        it_line, train_line, val_line = line.split('\\t')\n",
    "\n",
    "        it = int(it_line.strip('[]'))\n",
    "        train = float(train_line.split(':')[1])\n",
    "        val = float(val_line.split(':')[1])\n",
    "\n",
    "        results.append((it, train, val))\n",
    "    \n",
    "    columns = ['num_iter', 'train_auc', 'val_auc']\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "drop_features = 'job'\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 42, \n",
    "    'eval_metric': 'auc', \n",
    "    'n_jobs': -1,\n",
    "    'booster': 'gbtree', \n",
    "    'colsample_bytree': 0.4,\n",
    "    'learning_rate': 0.01, \n",
    "    'max_depth': 3, \n",
    "    'min_child_weight': 5, \n",
    "#     'n_estimators': 3000, \n",
    "    'objective': 'binary:logistic', \n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "\n",
    "num_cols = list(df_train_copy.columns[df_train_copy.dtypes != 'object'])\n",
    "X_train_num = poly.fit_transform(df_train_copy[num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy.drop(num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_train_poly.columns.values)\n",
    "df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "X_val_num = poly.transform(df_val_copy[num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy.drop(num_cols,axis=1,inplace=True)\n",
    "df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "new_features = list(df_train_copy.columns.values)\n",
    "dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "X_train_new = dv.fit_transform(dict_train_new)\n",
    "\n",
    "feature_names = dv.get_feature_names()\n",
    "dtrain = xgb.DMatrix(X_train_new,label=y_train,feature_names=feature_names)\n",
    "\n",
    "dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "X_val_new = dv.transform(dict_val_new)\n",
    "dval = xgb.DMatrix(X_val_new,label=y_val,feature_names=feature_names)\n",
    "\n",
    "watchlist = [(dtrain,'train'),(dval,'val')]\n",
    "\n",
    "model = xgb.train(xgb_params,dtrain,num_boost_round=3000,evals=watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = parse_xgb_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_score.num_iter, df_score.train_auc, label='train')\n",
    "plt.plot(df_score.num_iter, df_score.val_auc, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_score.num_iter, df_score.val_auc, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_score, x=\"num_iter\", y=\"val_auc\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Plotly graphs appear blank when viewing previously run notebook\n",
    "\n",
    "**Observations:** We can see that around iteration 335 the score is 0.79351, then it drops and then from iteration 1286 again it starts increasing upto iteration 2330 - where it is highest 0.79461 (but the increase is very minimal - 0.0011 compared to iter 335) and then its pretty stable. Thus, for practical purposes we will choose training upto iteration 335 as it will take lesser compute resources/time to train and still achieve optimal score possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.sort_values(by='val_auc',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a last training with iteration 335 and validation on validation dataset, before training the final model on full_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = 'job'\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 42, \n",
    "    'eval_metric': 'auc', \n",
    "    'n_jobs': -1,\n",
    "    'booster': 'gbtree', \n",
    "    'colsample_bytree': 0.4,\n",
    "    'learning_rate': 0.01, \n",
    "    'max_depth': 3, \n",
    "    'min_child_weight': 5, \n",
    "#     'n_estimators': 3000, \n",
    "    'objective': 'binary:logistic', \n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "df_val_copy = df_val.copy()\n",
    "df_val_copy.drop(drop_features,axis=1,inplace=True)\n",
    "\n",
    "num_cols = list(df_train_copy.columns[df_train_copy.dtypes != 'object'])\n",
    "X_train_num = poly.fit_transform(df_train_copy[num_cols])\n",
    "df_train_poly = pd.DataFrame(X_train_num, columns=[f\"poly_{i}\" for i in range(X_train_num.shape[1])])\n",
    "df_train_copy.drop(num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_train_poly.columns.values)\n",
    "df_train_copy[poly_cols] = df_train_poly[poly_cols]\n",
    "\n",
    "X_val_num = poly.transform(df_val_copy[num_cols])\n",
    "df_val_poly = pd.DataFrame(X_val_num, columns=[f\"poly_{i}\" for i in range(X_val_num.shape[1])])\n",
    "df_val_copy.drop(num_cols,axis=1,inplace=True)\n",
    "df_val_copy[poly_cols] = df_val_poly[poly_cols]\n",
    "\n",
    "new_features = list(df_train_copy.columns.values)\n",
    "dict_train_new = df_train_copy[new_features].to_dict(orient='records')\n",
    "X_train_new = dv.fit_transform(dict_train_new)\n",
    "\n",
    "feature_names = dv.get_feature_names()\n",
    "dtrain = xgb.DMatrix(X_train_new,label=y_train,feature_names=feature_names)\n",
    "\n",
    "dict_val_new = df_val_copy[new_features].to_dict(orient='records')\n",
    "X_val_new = dv.transform(dict_val_new)\n",
    "dval = xgb.DMatrix(X_val_new,label=y_val,feature_names=feature_names)\n",
    "\n",
    "model = xgb.train(xgb_params,dtrain,num_boost_round=335)\n",
    "\n",
    "y_pred = model.predict(dval)\n",
    "roc_auc_val = roc_auc_score(y_val,y_pred)\n",
    "roc_auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(dtrain)\n",
    "roc_auc_score(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at various metrics like tp, tn, fp, fn, Precision, Recall, F1 score, TRP, FPR, AUC, Accuracy etc. to detemine which threshold should be used to make the decision on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics_dataframe(y_val,y_pred):\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predicted_positive = (y_pred > t)\n",
    "        predicted_negative = (y_pred <= t)\n",
    "\n",
    "        tp = (actual_positive & predicted_positive).sum()\n",
    "        tn = (actual_negative & predicted_negative).sum()\n",
    "        fp = (predicted_positive & actual_negative).sum()\n",
    "        fn = (predicted_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold','tp','fp','fn','tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "    \n",
    "    #'precision', 'recall', 'f1', 'tpr', 'fpr', 'auc', 'accuracy'\n",
    "\n",
    "    df_scores['precision'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fp'])\n",
    "    df_scores['recall'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fn'])\n",
    "    df_scores['f1'] = 2 * (df_scores['precision'] * df_scores['recall']) / (df_scores['precision'] + df_scores['recall'])\n",
    "    df_scores['tpr'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fn'])\n",
    "    df_scores['fpr'] = df_scores['fp'] / (df_scores['fp'] + df_scores['tn'])\n",
    "    df_scores['tnr'] = df_scores['tn'] / (df_scores['tn'] + df_scores['fp'])\n",
    "    df_scores['fnr'] = df_scores['fn'] / (df_scores['fn'] + df_scores['tp'])\n",
    "    df_scores['auc'] = auc(df_scores['fpr'], df_scores['tpr'])\n",
    "    df_scores['accuracy'] = (df_scores['tp'] + df_scores['tn'])/(df_scores['tp'] + df_scores['fp'] + df_scores['tn'] + df_scores['fn'])\n",
    "        \n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = all_metrics_dataframe(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_all_scores, x=\"threshold\", y=[\"auc\",\"accuracy\",\"f1\",\"precision\",\"recall\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Plotly graphs appear blank when viewing previously run notebook\n",
    "\n",
    "**Observations:** In ideal scenario we would like our predictions to be always correct i.e. have fp and fn as 0. Practically it is not possible. We will see what threshold makes more sense to us to use for our predictions such that we have comparatively acceptable fp and fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_all_scores, x=\"threshold\", y=[\"fp\",\"fn\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Plotly graphs appear blank when viewing previously run notebook\n",
    "\n",
    "**Observations:** \n",
    "* We can see that for threshold of 0, we have 0 fn but then we have 7260 fp (meaning we would be predicting that all customers will subscribe to Term deposit - which obviously is wrong). At the extreme right end where threshold is close to 1, we have 0 fp but 896 fn (meaning we would be predicting that none of the customers will be making Term deposit).\n",
    "* At a threshold of 0.32 we see fp and fn crossing - where fp is around 440 and fn is around 487."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_all_scores, x=\"fpr\", y=\"tpr\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the F1 scores and at what threshold is our F1 score the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_dataframe(y_val,y_pred):\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predicted_positive = (y_pred >= t)\n",
    "        predicted_negative = (y_pred < t)\n",
    "\n",
    "        tp = (actual_positive & predicted_positive).sum()\n",
    "        tn = (actual_negative & predicted_negative).sum()\n",
    "        fp = (predicted_positive & actual_negative).sum()\n",
    "        fn = (predicted_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold','tp','fp','fn','tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    df_scores['precision'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fp'])\n",
    "    df_scores['recall'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fn'])\n",
    "    df_scores['f1'] = 2 * (df_scores['precision'] * df_scores['recall']) / (df_scores['precision'] + df_scores['recall'])\n",
    "        \n",
    "    return df_scores\n",
    "\n",
    "df_f1_scores = f1_dataframe(y_val,y_pred)\n",
    "\n",
    "print(df_f1_scores[df_f1_scores['f1'] == df_f1_scores['f1'].max()][['threshold','f1']])\n",
    "\n",
    "plt.plot(df_f1_scores['threshold'], df_f1_scores['f1'], label='F1 score')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('F1 score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the TPR and FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr_dataframe(y_val,y_pred):\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predicted_positive = (y_pred >= t)\n",
    "        predicted_negative = (y_pred < t)\n",
    "\n",
    "        tp = (actual_positive & predicted_positive).sum()\n",
    "        tn = (actual_negative & predicted_negative).sum()\n",
    "        fp = (predicted_positive & actual_negative).sum()\n",
    "        fn = (predicted_negative & actual_positive).sum()\n",
    "\n",
    "    #     tpr = tp / (tp + fn)\n",
    "    #     fpr = fp / (fp + tn)\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold','tp','fp','fn','tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    df_scores['tpr'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fn'])\n",
    "    df_scores['fpr'] = df_scores['fp'] / (df_scores['fp'] + df_scores['tn'])\n",
    "        \n",
    "    return df_scores\n",
    "\n",
    "df_tpr_fpr_scores = tpr_fpr_dataframe(y_val,y_pred)\n",
    "\n",
    "# print(round(auc(df_tpr_fpr_scores['fpr'],df_tpr_fpr_scores['tpr']),3))\n",
    "\n",
    "plt.plot(df_tpr_fpr_scores['threshold'], df_tpr_fpr_scores['tpr'], label='TPR')\n",
    "plt.plot(df_tpr_fpr_scores['threshold'], df_tpr_fpr_scores['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Precision and Recall for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_dataframe(y_val,y_pred):\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predicted_positive = (y_pred >= t)\n",
    "        predicted_negative = (y_pred < t)\n",
    "\n",
    "        tp = (actual_positive & predicted_positive).sum()\n",
    "        tn = (actual_negative & predicted_negative).sum()\n",
    "        fp = (predicted_positive & actual_negative).sum()\n",
    "        fn = (predicted_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold','tp','fp','fn','tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    df_scores['precision'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fp'])\n",
    "    df_scores['recall'] = df_scores['tp'] / (df_scores['tp'] + df_scores['fn'])\n",
    "        \n",
    "    return df_scores\n",
    "\n",
    "df_pr_rec_scores = precision_recall_dataframe(y_val,y_pred)\n",
    "\n",
    "plt.plot(df_pr_rec_scores['threshold'], df_pr_rec_scores['precision'], label='Precision')\n",
    "plt.plot(df_pr_rec_scores['threshold'], df_pr_rec_scores['recall'], label='Recall')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('precision/recall')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_pr_rec_scores, x=\"threshold\", y=[\"precision\",\"recall\"])\n",
    "# fig = px.line(df_pr_rec_scores, x=\"threshold\", y=\"recall\",labels='Recall')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Vs Recall\n",
    "plt.plot(df_pr_rec_scores['recall'], df_pr_rec_scores['precision'], label='Precision Vs Recall')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpr_fpr_scores[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(df_tpr_fpr_scores['fpr'],df_tpr_fpr_scores['tpr'],label='model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train-final-model'></a>\n",
    "#### 6.2 Train final model\n",
    "[back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on full_train dataset for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = 'job'\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 42, \n",
    "    'eval_metric': 'auc', \n",
    "    'n_jobs': -1,\n",
    "    'booster': 'gbtree', \n",
    "    'colsample_bytree': 0.4,\n",
    "    'learning_rate': 0.01, \n",
    "    'max_depth': 3, \n",
    "    'min_child_weight': 5, \n",
    "#     'n_estimators': 3000, \n",
    "    'objective': 'binary:logistic', \n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "df_full_train_copy = df_full_train.copy()\n",
    "df_full_train_copy.drop(drop_features,axis=1,inplace=True)\n",
    "y_full_train = df_full_train_copy['y']\n",
    "del df_full_train_copy['y']\n",
    "\n",
    "df_test_copy = df_test.copy()\n",
    "df_test_copy.drop(drop_features,axis=1,inplace=True)\n",
    "\n",
    "num_cols = list(df_full_train_copy.columns[df_full_train_copy.dtypes != 'object'])\n",
    "X_full_train_num = poly.fit_transform(df_full_train_copy[num_cols])\n",
    "df_full_train_poly = pd.DataFrame(X_full_train_num, columns=[f\"poly_{i}\" for i in range(X_full_train_num.shape[1])])\n",
    "df_full_train_copy.drop(num_cols,axis=1,inplace=True)\n",
    "poly_cols = list(df_full_train_poly.columns.values)\n",
    "df_full_train_copy[poly_cols] = df_full_train_poly[poly_cols]\n",
    "\n",
    "X_test_num = poly.transform(df_test_copy[num_cols])\n",
    "df_test_poly = pd.DataFrame(X_test_num, columns=[f\"poly_{i}\" for i in range(X_test_num.shape[1])])\n",
    "df_test_copy.drop(num_cols,axis=1,inplace=True)\n",
    "df_test_copy[poly_cols] = df_test_poly[poly_cols]\n",
    "\n",
    "new_features = list(df_full_train_copy.columns.values)\n",
    "dict_full_train_new = df_full_train_copy[new_features].to_dict(orient='records')\n",
    "X_full_train_new = dv.fit_transform(dict_full_train_new)\n",
    "\n",
    "feature_names = dv.get_feature_names()\n",
    "dfulltrain = xgb.DMatrix(X_full_train_new,label=y_full_train,feature_names=feature_names)\n",
    "\n",
    "dict_test_new = df_test_copy[new_features].to_dict(orient='records')\n",
    "X_test_new = dv.transform(dict_test_new)\n",
    "dtest = xgb.DMatrix(X_test_new,label=y_test,feature_names=feature_names)\n",
    "\n",
    "model = xgb.train(xgb_params,dfulltrain,num_boost_round=335)\n",
    "\n",
    "y_pred_test = model.predict(dtest)\n",
    "roc_auc_test = roc_auc_score(y_test,y_pred_test)\n",
    "roc_auc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_file = f'xgb_model.bin'\n",
    "\n",
    "with open(model_output_file,'wb') as f_out:\n",
    "    pickle.dump((poly,dv,model),f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
